{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# 1. Load CSV data and preprocess it.\n",
    "df = pd.read_csv(\"restaurant_data.csv\")  # Update the file path as needed\n",
    "\n",
    "# Function to convert a CSV row into a descriptive text passage.\n",
    "def create_text(row):\n",
    "    # Adjust the column names to match those in your CSV.\n",
    "    text = (\n",
    "        f\"Restaurant: {row['restaurant_name']}. \"\n",
    "        f\"City: {row['city']}. \"\n",
    "        f\"Dishes: {row['dishes']}. \"\n",
    "        f\"Description: {row.get('description', '')}\"\n",
    "    )\n",
    "    return text\n",
    "\n",
    "# Convert each row into a text passage.\n",
    "passages = df.apply(create_text, axis=1).tolist()\n",
    "\n",
    "# 2. Create embeddings for the passages using SentenceTransformer.\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = embedder.encode(passages, convert_to_numpy=True)\n",
    "\n",
    "# 3. Build a FAISS index for efficient vector similarity search.\n",
    "embedding_dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(embedding_dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "# Retrieval function: Given a query, returns the top k most relevant passages.\n",
    "def retrieve(query, k=5):\n",
    "    query_embedding = embedder.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    # Retrieve the corresponding passages.\n",
    "    results = [passages[i] for i in indices[0] if i < len(passages)]\n",
    "    return results\n",
    "\n",
    "# 4. Initialize DeepSeek R1 via Hugging Face Transformers.\n",
    "# Note: Ensure that \"deepseek/r1\" is the correct model identifier.\n",
    "model_name = \"deepseek/r1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Generation function: Uses DeepSeek R1 to generate an answer based on the query and retrieved context.\n",
    "def generate_answer(query, context_passages):\n",
    "    # Combine retrieved passages into a single context text.\n",
    "    context_text = \"\\n\".join(context_passages)\n",
    "    prompt = (\n",
    "        f\"Using the following context information about restaurants:\\n{context_text}\\n\\n\"\n",
    "        f\"Answer the following question:\\n{query}\\n\"\n",
    "    )\n",
    "    \n",
    "    # Generate an answer using DeepSeek R1.\n",
    "    output = generator(prompt, max_length=200, do_sample=True, temperature=0.7)\n",
    "    answer = output[0]['generated_text']\n",
    "    return answer\n",
    "\n",
    "# Function that ties together retrieval and generation.\n",
    "def answer_question(query):\n",
    "    retrieved_context = retrieve(query, k=5)\n",
    "    answer = generate_answer(query, retrieved_context)\n",
    "    return answer\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    query1 = \"Which restaurants in Los Angeles offer dishes with Impossible Meat?\"\n",
    "    answer1 = answer_question(query1)\n",
    "    print(\"Answer 1:\")\n",
    "    print(answer1)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    query2 = \"Give me a summary of the latest trends around desserts in San Francisco.\"\n",
    "    answer2 = answer_question(query2)\n",
    "    print(\"Answer 2:\")\n",
    "    print(answer2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
