{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3f/x95cmnkn57s5pfth5lygr5k00000gn/T/ipykernel_49988/3649054773.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m HuggingFaceEmbeddings(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBAAI/bge-large-en\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Store embeddings in FAISS for efficient retrieval\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_documents(\u001b[43mdocuments\u001b[49m, embedding_model)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Save FAISS index for later use\u001b[39;00m\n\u001b[1;32m     12\u001b[0m vector_store\u001b[38;5;241m.\u001b[39msave_local(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./faiss_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'documents' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize HuggingFace embeddings (uses a model similar to Sentence Transformers)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en\")\n",
    "\n",
    "# Store embeddings in FAISS for efficient retrieval\n",
    "vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "\n",
    "# Save FAISS index for later use\n",
    "vector_store.save_local(\"./faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Title: Dick Johnson Is Dead\n",
      "        Type: Movie\n",
      "        Director: Kirsten Johnson\n",
      "        Cast: Unknown\n",
      "        Country: United States\n",
      "        Release Year: 2020\n",
      "        Rating: PG-13\n",
      "        Duration: 90 min\n",
      "        Genres: Documentaries\n",
      "        Description: As her father nears the end of his life, filmmaker Kirsten Johnson stages his death in inventive and comical ways to help them both face the inevitable.' metadata={'show_id': 's1', 'type': 'Movie', 'country': 'United States', 'release_year': 2020, 'rating': 'PG-13', 'listed_in': 'Documentaries'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "import pandas as pd\n",
    "from langchain.schema import Document\n",
    "\n",
    "# def custom_csv_loader(file_path):\n",
    "#     \"\"\"\n",
    "#     Custom function to load a CSV file, format each row into structured text, \n",
    "#     and return a list of LangChain Document objects with relevant metadata.\n",
    "\n",
    "#     :param file_path: Path to the CSV file\n",
    "#     :return: List of LangChain Document objects\n",
    "#     \"\"\"\n",
    "#     # Load CSV into Pandas DataFrame\n",
    "#     df = pd.read_csv(file_path)\n",
    "    \n",
    "#     documents = []\n",
    "    \n",
    "#     for index, row in df.iterrows():\n",
    "#         # Convert row into structured text format\n",
    "#         text_representation = f\"\"\"\n",
    "#         Customer ID: {row['Customer Id']}\n",
    "#         Name: {row['First Name']} {row['Last Name']}\n",
    "#         Company: {row['Company']}\n",
    "#         City: {row['City']}\n",
    "#         Country: {row['Country']}\n",
    "#         Phone 1: {row['Phone 1']}\n",
    "#         Phone 2: {row['Phone 2']}\n",
    "#         Email: {row['Email']}\n",
    "#         Subscription Date: {row['Subscription Date']}\n",
    "#         Website: {row['Website']}\n",
    "#         \"\"\"\n",
    "        \n",
    "#         # Metadata excluding file path but adding row number and Customer ID\n",
    "#         metadata = {\n",
    "#             \"row_number\": index + 1,\n",
    "#             \"customer_id\": row[\"Customer Id\"],\n",
    "#             \"city\": row[\"City\"],\n",
    "#             \"country\": row[\"Country\"]\n",
    "#         }\n",
    "\n",
    "#         # Create a Document object\n",
    "#         document = Document(page_content=text_representation.strip(), metadata=metadata)\n",
    "#         documents.append(document)\n",
    "    \n",
    "#     return documents\n",
    "\n",
    "\n",
    "def custom_csv_loader(file_path):\n",
    "    \"\"\"\n",
    "    Converts CSV data into structured text documents with metadata for RAG.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    documents = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        text_representation = f\"\"\"\n",
    "        Title: {row['title']}\n",
    "        Type: {row['type']}\n",
    "        Director: {row['director'] if pd.notna(row['director']) else \"Unknown\"}\n",
    "        Cast: {row['cast'] if pd.notna(row['cast']) else \"Unknown\"}\n",
    "        Country: {row['country'] if pd.notna(row['country']) else \"Unknown\"}\n",
    "        Release Year: {row['release_year']}\n",
    "        Rating: {row['rating']}\n",
    "        Duration: {row['duration']}\n",
    "        Genres: {row['listed_in']}\n",
    "        Description: {row['description']}\n",
    "        \"\"\"\n",
    "\n",
    "        metadata = {\n",
    "            \"show_id\": row[\"show_id\"],\n",
    "            \"type\": row[\"type\"],\n",
    "            \"country\": row[\"country\"] if pd.notna(row[\"country\"]) else \"Unknown\",\n",
    "            \"release_year\": row[\"release_year\"],\n",
    "            \"rating\": row[\"rating\"],\n",
    "            \"listed_in\": row[\"listed_in\"]\n",
    "        }\n",
    "\n",
    "        document = Document(page_content=text_representation.strip(), metadata=metadata)\n",
    "        documents.append(document)\n",
    "\n",
    "    return documents\n",
    "\n",
    "# Example usage\n",
    "# file_path = \"./customers-100.csv\"\n",
    "file_path = \"./netflix_titles.csv\"\n",
    "documents = custom_csv_loader(file_path)\n",
    "\n",
    "# Display first document for verification\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "# embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # You can change the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize HuggingFace embeddings (uses a model similar to Sentence Transformers)\n",
    "# embedding_model = HuggingFaceEmbeddings(model_name=\"sentencetransformers/all-MiniLM-L6-v2\")\n",
    "# embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en\")\n",
    "\n",
    "# Store embeddings in FAISS for efficient retrieval\n",
    "vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "\n",
    "# Save FAISS index for later use\n",
    "vector_store.save_local(\"./faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# url = \"https://huggingface.co\"\n",
    "# try:\n",
    "#     response = requests.get(url, timeout=5)\n",
    "#     print(f\"Status Code: {response.status_code}\")\n",
    "#     if response.status_code == 200:\n",
    "#         print(\"✅ Hugging Face is reachable!\")\n",
    "#     else:\n",
    "#         print(\"⚠️ Unable to connect to Hugging Face.\")\n",
    "# except requests.ConnectionError:\n",
    "#     print(\"❌ No internet connection or Hugging Face is blocked.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Document:\n",
      "Title: Amit Tandon: Family Tandoncies\n",
      "        Type: Movie\n",
      "        Director: Unknown\n",
      "        Cast: Amit Tandon\n",
      "        Country: India\n",
      "        Release Year: 2019\n",
      "        Rating: TV-14\n",
      "        Duration: 72 min\n",
      "        Genres: Stand-Up Comedy\n",
      "        Description: From the death of romance in marriage to the injustices of modern-day parenting, Amit Tandon shares wisdom and wisecracks as a battle-scarred family guy.\n",
      "Metadata: {'show_id': 's2870', 'type': 'Movie', 'country': 'India', 'release_year': 2019, 'rating': 'TV-14', 'listed_in': 'Stand-Up Comedy'}\n",
      "--------------------------------------------------\n",
      "Retrieved Document:\n",
      "Title: Your Excellency\n",
      "        Type: Movie\n",
      "        Director: Funke Akindele\n",
      "        Cast: Akin Lewis, Funke Akindele, Kemi Lala Akindoju, Shaffy Bello, Kunle Coker, Eku Edewor, Alexx Ekubo, Osas Ighodaro Ajibade, Seyi Law, Falz, Chigul, Deyemi Okanlawon, Beverly Osu, Toni Tones, Christian Paul, Bimbo Manuel, Helen Paul\n",
      "        Country: Unknown\n",
      "        Release Year: 2019\n",
      "        Rating: TV-G\n",
      "        Duration: 120 min\n",
      "        Genres: Comedies, International Movies\n",
      "        Description: Bumbling through politics, a billionaire businessman's presidential campaign seems destined for disaster until it gets a boost from social media.\n",
      "Metadata: {'show_id': 's2253', 'type': 'Movie', 'country': 'Unknown', 'release_year': 2019, 'rating': 'TV-G', 'listed_in': 'Comedies, International Movies'}\n",
      "--------------------------------------------------\n",
      "Retrieved Document:\n",
      "Title: Fadily Camara : La plus drôle de tes copines\n",
      "        Type: Movie\n",
      "        Director: Gautier & Leduc\n",
      "        Cast: Fadily Camara\n",
      "        Country: France\n",
      "        Release Year: 2019\n",
      "        Rating: TV-MA\n",
      "        Duration: 54 min\n",
      "        Genres: Stand-Up Comedy\n",
      "        Description: Irrepressible French comedian Fadily Camara weaves jokes, vivid characters and physical comedy into a lively stand-up show at La Cigale in Paris.\n",
      "Metadata: {'show_id': 's3286', 'type': 'Movie', 'country': 'France', 'release_year': 2019, 'rating': 'TV-MA', 'listed_in': 'Stand-Up Comedy'}\n",
      "--------------------------------------------------\n",
      "Retrieved Document:\n",
      "Title: Something Huge\n",
      "        Type: Movie\n",
      "        Director: Carlo Padial\n",
      "        Cast: Berto Romero, Carolina Bang, Javier Botet, Carlos Areces, Miguel Noguera, Toni Sevilla\n",
      "        Country: Spain\n",
      "        Release Year: 2017\n",
      "        Rating: TV-MA\n",
      "        Duration: 87 min\n",
      "        Genres: Comedies, Independent Movies, International Movies\n",
      "        Description: A director and a comedian want to shoot the \"comedy of the future\" and make cinema history, but soon run out of resources and run into mission creep.\n",
      "Metadata: {'show_id': 's5014', 'type': 'Movie', 'country': 'Spain', 'release_year': 2017, 'rating': 'TV-MA', 'listed_in': 'Comedies, Independent Movies, International Movies'}\n",
      "--------------------------------------------------\n",
      "Retrieved Document:\n",
      "Title: Michelle Wolf: Joke Show\n",
      "        Type: Movie\n",
      "        Director: Lance Bangs\n",
      "        Cast: Michelle Wolf\n",
      "        Country: United States\n",
      "        Release Year: 2019\n",
      "        Rating: TV-MA\n",
      "        Duration: 60 min\n",
      "        Genres: Stand-Up Comedy\n",
      "        Description: Comedian Michelle Wolf takes on outrage culture, massages, childbirth, feminism and much more (like otters) in a stand-up special from New York City.\n",
      "Metadata: {'show_id': 's3161', 'type': 'Movie', 'country': 'United States', 'release_year': 2019, 'rating': 'TV-MA', 'listed_in': 'Stand-Up Comedy'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load FAISS index\n",
    "vector_store = FAISS.load_local(\"./faiss_index\", embedding_model,allow_dangerous_deserialization=True)\n",
    "# FAISS serialization in LangChain uses pickle, which could be exploited if loading from an untrusted source.\n",
    "\n",
    "# Query example\n",
    "query = \"Name a comedy movie released in 2019\"\n",
    "retrieved_docs = vector_store.similarity_search(query, k=5)  # Retrieve top-3 matches\n",
    "\n",
    "# Display retrieved results\n",
    "for doc in retrieved_docs:\n",
    "    print(\"Retrieved Document:\")\n",
    "    print(doc.page_content)\n",
    "    print(\"Metadata:\", doc.metadata)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix = pd.read_csv(\"./netflix_titles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>show_id</th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>director</th>\n",
       "      <th>cast</th>\n",
       "      <th>country</th>\n",
       "      <th>date_added</th>\n",
       "      <th>release_year</th>\n",
       "      <th>rating</th>\n",
       "      <th>duration</th>\n",
       "      <th>listed_in</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s1</td>\n",
       "      <td>Movie</td>\n",
       "      <td>Dick Johnson Is Dead</td>\n",
       "      <td>Kirsten Johnson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>September 25, 2021</td>\n",
       "      <td>2020</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>90 min</td>\n",
       "      <td>Documentaries</td>\n",
       "      <td>As her father nears the end of his life, filmm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s2</td>\n",
       "      <td>TV Show</td>\n",
       "      <td>Blood &amp; Water</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ama Qamata, Khosi Ngema, Gail Mabalane, Thaban...</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>September 24, 2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>2 Seasons</td>\n",
       "      <td>International TV Shows, TV Dramas, TV Mysteries</td>\n",
       "      <td>After crossing paths at a party, a Cape Town t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s3</td>\n",
       "      <td>TV Show</td>\n",
       "      <td>Ganglands</td>\n",
       "      <td>Julien Leclercq</td>\n",
       "      <td>Sami Bouajila, Tracy Gotoas, Samuel Jouy, Nabi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>September 24, 2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>1 Season</td>\n",
       "      <td>Crime TV Shows, International TV Shows, TV Act...</td>\n",
       "      <td>To protect his family from a powerful drug lor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s4</td>\n",
       "      <td>TV Show</td>\n",
       "      <td>Jailbirds New Orleans</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>September 24, 2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>1 Season</td>\n",
       "      <td>Docuseries, Reality TV</td>\n",
       "      <td>Feuds, flirtations and toilet talk go down amo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s5</td>\n",
       "      <td>TV Show</td>\n",
       "      <td>Kota Factory</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mayur More, Jitendra Kumar, Ranjan Raj, Alam K...</td>\n",
       "      <td>India</td>\n",
       "      <td>September 24, 2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>2 Seasons</td>\n",
       "      <td>International TV Shows, Romantic TV Shows, TV ...</td>\n",
       "      <td>In a city of coaching centers known to train I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  show_id     type                  title         director  \\\n",
       "0      s1    Movie   Dick Johnson Is Dead  Kirsten Johnson   \n",
       "1      s2  TV Show          Blood & Water              NaN   \n",
       "2      s3  TV Show              Ganglands  Julien Leclercq   \n",
       "3      s4  TV Show  Jailbirds New Orleans              NaN   \n",
       "4      s5  TV Show           Kota Factory              NaN   \n",
       "\n",
       "                                                cast        country  \\\n",
       "0                                                NaN  United States   \n",
       "1  Ama Qamata, Khosi Ngema, Gail Mabalane, Thaban...   South Africa   \n",
       "2  Sami Bouajila, Tracy Gotoas, Samuel Jouy, Nabi...            NaN   \n",
       "3                                                NaN            NaN   \n",
       "4  Mayur More, Jitendra Kumar, Ranjan Raj, Alam K...          India   \n",
       "\n",
       "           date_added  release_year rating   duration  \\\n",
       "0  September 25, 2021          2020  PG-13     90 min   \n",
       "1  September 24, 2021          2021  TV-MA  2 Seasons   \n",
       "2  September 24, 2021          2021  TV-MA   1 Season   \n",
       "3  September 24, 2021          2021  TV-MA   1 Season   \n",
       "4  September 24, 2021          2021  TV-MA  2 Seasons   \n",
       "\n",
       "                                           listed_in  \\\n",
       "0                                      Documentaries   \n",
       "1    International TV Shows, TV Dramas, TV Mysteries   \n",
       "2  Crime TV Shows, International TV Shows, TV Act...   \n",
       "3                             Docuseries, Reality TV   \n",
       "4  International TV Shows, Romantic TV Shows, TV ...   \n",
       "\n",
       "                                         description  \n",
       "0  As her father nears the end of his life, filmm...  \n",
       "1  After crossing paths at a party, a Cape Town t...  \n",
       "2  To protect his family from a powerful drug lor...  \n",
       "3  Feuds, flirtations and toilet talk go down amo...  \n",
       "4  In a city of coaching centers known to train I...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8807"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(netflix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "prompt = f\"\"\"\n",
    "You are a helpful assistant. Answer the question using the provided information.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, pipeline\n",
    "import torch\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# ✅ Load Model with Accelerate's Device Mapping\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use BF16 on CUDA, FP32 otherwise\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Automatically distribute across available GPUs/CPUs\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# ✅ Set Up Generation Config\u001b[39;00m\n\u001b[1;32m     23\u001b[0m generation_config \u001b[38;5;241m=\u001b[39m GenerationConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "File \u001b[0;32m~/Desktop/MenuData/RestaurantChatbot/venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/MenuData/RestaurantChatbot/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:3535\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3531\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3532\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeepSpeed Zero-3 is not compatible with `low_cpu_mem_usage=True` or with passing a `device_map`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3533\u001b[0m         )\n\u001b[1;32m   3534\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 3535\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   3536\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3537\u001b[0m         )\n\u001b[1;32m   3539\u001b[0m \u001b[38;5;66;03m# handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\u001b[39;00m\n\u001b[1;32m   3540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit \u001b[38;5;129;01mor\u001b[39;00m load_in_8bit:\n",
      "\u001b[0;31mImportError\u001b[0m: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "import torch\n",
    "from accelerate import infer_auto_device_map\n",
    "\n",
    "# ✅ Define Model Name\n",
    "model_name = \"deepseek-ai/deepseek-llm-7b-base\"\n",
    "\n",
    "# ✅ Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# ✅ Ensure Compatibility with Hardware\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ✅ Load Model with Accelerate's Device Mapping\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,  # Use BF16 on CUDA, FP32 otherwise\n",
    "    device_map=\"auto\"  # Automatically distribute across available GPUs/CPUs\n",
    ")\n",
    "\n",
    "# ✅ Set Up Generation Config\n",
    "generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "generation_config.pad_token_id = generation_config.eos_token_id\n",
    "\n",
    "print(\"✅ DeepSeek LLM 7B Loaded Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Accelerate is installed and working!\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "print(\"✅ Accelerate is installed and working!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1117021010.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[37], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    accelerate config\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "accelerate config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ CPU Performance Warning\n",
    "DeepSeek LLM 7B is very large (~13GB RAM required).\n",
    "Running it on Intel CPU will be extremely slow.\n",
    "For better performance, consider:\n",
    "Using a smaller model (e.g., \"deepseek-ai/deepseek-llm-7b-instruct\").\n",
    "Running on Google Colab with an A100 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Model path does not exist: ~/models/mistral-7b.Q4_K_M.gguf",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m~/models/mistral-7b.Q4_K_M.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# ✅ Load model with optimized CPU settings\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLlama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_ctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 6 threads for your 6-core CPU\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# ✅ Test inference\u001b[39;00m\n\u001b[1;32m     10\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the capital of France?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/MenuData/RestaurantChatbot/venv/lib/python3.11/site-packages/llama_cpp/llama.py:368\u001b[0m, in \u001b[0;36mLlama.__init__\u001b[0;34m(self, model_path, n_gpu_layers, split_mode, main_gpu, tensor_split, rpc_servers, vocab_only, use_mmap, use_mlock, kv_overrides, seed, n_ctx, n_batch, n_ubatch, n_threads, n_threads_batch, rope_scaling_type, pooling_type, rope_freq_base, rope_freq_scale, yarn_ext_factor, yarn_attn_factor, yarn_beta_fast, yarn_beta_slow, yarn_orig_ctx, logits_all, embedding, offload_kqv, flash_attn, no_perf, last_n_tokens_size, lora_base, lora_scale, lora_path, numa, chat_format, chat_handler, draft_model, tokenizer, type_k, type_v, spm_infill, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspm_infill \u001b[38;5;241m=\u001b[39m spm_infill\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_path):\n\u001b[0;32m--> 368\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel path does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[1;32m    371\u001b[0m     contextlib\u001b[38;5;241m.\u001b[39mclosing(\n\u001b[1;32m    372\u001b[0m         internals\u001b[38;5;241m.\u001b[39mLlamaModel(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    377\u001b[0m     )\n\u001b[1;32m    378\u001b[0m )\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# Override tokenizer\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Model path does not exist: ~/models/mistral-7b.Q4_K_M.gguf"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# ✅ Update this path with the downloaded GGUF model\n",
    "model_path = \"~/models/mistral-7b.Q4_K_M.gguf\"\n",
    "\n",
    "# ✅ Load model with optimized CPU settings\n",
    "llm = Llama(model_path=model_path, n_ctx=2048, n_threads=6)  # 6 threads for your 6-core CPU\n",
    "\n",
    "# ✅ Test inference\n",
    "query = \"What is the capital of France?\"\n",
    "response = llm(f\"Answer the following question:\\n{query}\")\n",
    "print(response[\"choices\"][0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3621e1c5fe42389ca04ce57faacb7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral-7b-instruct-v0.1.Q4_K_M.gguf:   0%|          | 0.00/4.37G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path: /Users/gauravbindra/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.1-GGUF/snapshots/731a9fc8f06f5f5e2db8a0cf9d256197eb6e05d1/mistral-7b-instruct-v0.1.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Replace with the exact filename from the GGUF model page\n",
    "model_path = hf_hub_download(repo_id=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\", filename=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\")\n",
    "\n",
    "print(\"Model path:\", model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (AMD Radeon Pro 5300M) - 465 MiB free\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/gauravbindra/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.1-GGUF/snapshots/731a9fc8f06f5f5e2db8a0cf9d256197eb6e05d1/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors: offloading 0 repeating layers to GPU\n",
      "load_tensors: offloaded 0/33 layers to GPU\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 2048\n",
      "llama_init_from_model: n_ctx_per_seq = 2048\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Intel(R) UHD Graphics 630\n",
      "ggml_metal_init: found device: AMD Radeon Pro 5300M\n",
      "ggml_metal_init: picking default device: AMD Radeon Pro 5300M\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   AMD Radeon Pro 5300M\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = false\n",
      "ggml_metal_init: has residency sets    = false\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = false\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  =  4278.19 MB\n",
      "ggml_metal_init: loaded kernel_add                                 0x7fd2f81409f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                             0x7fd2f8141130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                 0x7fd2bfa76340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                             0x7fd2f82ed770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                 0x7fd23d59e300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                             0x7fd2f82c8020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                 0x7fd2f82fd5d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                             0x7fd2f813ed70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                          0x7fd2f813fdf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                          0x7fd23d56f0b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                          0x7fd2f8148330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                          0x7fd2bfa7f140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                               0x7fd2bfa7f900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                             0x7fd2b5c0f9e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                               0x7fd2c9fab500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                0x7fd2f82fed40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                0x7fd2c9fabbe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                             0x7fd2d4aeafe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                0x7fd2f82ff7b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                              0x7fd2dae277e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                          0x7fd2f82d4610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                        0x7fd2c9facc70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                0x7fd2f82c6460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                              0x7fd2f82bea60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                 0x7fd2f82d1a60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                        0x7fd23d5db3d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                      0x7fd2f82c1590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                        0x7fd2ad25f800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                      0x7fd2f82d6990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                       0x7fd2ad262b80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                     0x7fd2ad2646a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                        0x7fd2c5270de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                        0x7fd2d4aea360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                       0x7fd23d52e510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                       0x7fd2f82d70d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                       0x7fd23d52ef80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                       0x7fd2f82ade70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                       0x7fd2a97e8970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                       0x7fd2f82c4580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                       0x7fd2f82bba70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                       0x7fd2a97e59c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                       0x7fd23d52f9f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                       0x7fd23d530460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                    0x7fd2f82d84d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                     0x7fd2bd8b42a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                    0x7fd2d9ea1680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                      0x7fd2bdf87350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                      0x7fd2f82d9960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                      0x7fd2bd8a89f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                      0x7fd2f8290af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                     0x7fd2bd8a9c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                     0x7fd2bdf88fc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                        0x7fd2dae937d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                            0x7fd2bdf880d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                          0x7fd2f82ccb70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                0x7fd2d312f1b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                        0x7fd2f82ef230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                        0x7fd2bdf86460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                      0x7fd2dac65f50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                      0x7fd2d312d900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                 0x7fd2bd8ad5b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                   0x7fd2f82dee30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                      0x7fd23d530ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                     0x7fd2dae08c10 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                     0x7fd23d531940 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                     0x7fd2f82b98f0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                     0x7fd2f82f2200 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                     0x7fd2f82c51a0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2             0x7fd23d5323b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3             0x7fd2d0e0ab60 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4             0x7fd2f82d79f0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5             0x7fd2dae95530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2            0x7fd2dae90610 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3            0x7fd2d0e0b2a0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4            0x7fd23d532e20 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5            0x7fd2c724a7c0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2            0x7fd23a604080 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3            0x7fd23d533890 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4            0x7fd2f82fc060 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5            0x7fd2f82fc7a0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2            0x7fd23b2c51f0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3            0x7fd2e2bf5510 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4            0x7fd2e1949130 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5            0x7fd2e2bf9040 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2            0x7fd2b41a3610 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3            0x7fd23a604e80 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4            0x7fd23d949b10 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5            0x7fd2c857bb20 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2            0x7fd23a605560 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3            0x7fd2c8567c50 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4            0x7fd2cb2ae0d0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5            0x7fd2a13d7ca0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2            0x7fd2a13e3990 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3            0x7fd2a13a2c70 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4            0x7fd2cb29e440 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5            0x7fd2a13d62c0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2            0x7fd23a605fd0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3            0x7fd23a606710 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4            0x7fd2a13c1620 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5            0x7fd2a13e76e0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2            0x7fd2a13c8940 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3            0x7fd23a607180 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4            0x7fd2a13f7e40 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5            0x7fd2a13d2c40 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2          0x7fd2e1948c70 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3          0x7fd2a13f9f80 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4          0x7fd23a607bf0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5          0x7fd2a13a4fe0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                     0x7fd23d95c0e0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                     0x7fd23a608660 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                     0x7fd2e19fc120 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                     0x7fd2d6aa17d0 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                     0x7fd2d98df5a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                  0x7fd2a13ff9d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                   0x7fd23d95c7c0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                  0x7fd23d9fd590 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                    0x7fd2a13ac960 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                    0x7fd2a13ad3d0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                    0x7fd2d6a8e6e0 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                    0x7fd23d95adc0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                   0x7fd2a13b1400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                   0x7fd2c9834730 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                   0x7fd2a13d9490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                   0x7fd23a6090d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                  0x7fd23a609b40 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                  0x7fd2d98f8b10 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                  0x7fd2a13b4860 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                  0x7fd23a60a940 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                  0x7fd23d534300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                  0x7fd2d98e2490 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                  0x7fd23d5350a0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                  0x7fd23d9a1fb0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                  0x7fd23a60b740 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                  0x7fd23a60be20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32               0x7fd23e534200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                0x7fd2d98c7430 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32               0x7fd23a60c890 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                 0x7fd23a60d300 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                 0x7fd2d98fa020 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                 0x7fd2ca49cf20 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                 0x7fd23a8e06a0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                0x7fd23d9a26f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                0x7fd23a60dd70 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_f32_f32                    (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_f16_f32                    (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q4_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q4_1_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q5_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q5_1_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q8_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q2_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q3_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q4_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q5_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q6_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq2_xxs_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq2_xs_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq3_xxs_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq3_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq2_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq1_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq1_m_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq4_nl_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq4_xs_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_f32_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_f16_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q4_0_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q4_1_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q5_0_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q5_1_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q8_0_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q2_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q3_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q4_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q5_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q6_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq2_xxs_f32             (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq2_xs_f32              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq3_xxs_f32             (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq3_s_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq2_s_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq1_s_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq1_m_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq4_nl_f32              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq4_xs_f32              (not supported)\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                       0x7fd23a86e840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                       0x7fd23a60e7e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                       0x7fd23e534fa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                       0x7fd23a87f3a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                          0x7fd23a526720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                          0x7fd23a60f250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                      0x7fd23d9a34f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                      0x7fd23a8cc150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32           0x7fd23a527500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32           0x7fd23a8ccf10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                         0x7fd23e55e450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                             0x7fd23a60fcc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                  0x7fd23a5288f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32              0x7fd23a529dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                          0x7fd23a88e8a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                 0x7fd23a88efe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                0x7fd23d535b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                      0x7fd23a610730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h64            (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h80            (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h96            (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h112           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h128           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h256           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128         0x7fd23e55eb90 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128        0x7fd23a6111a0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128        0x7fd23a52b170 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128        0x7fd23d536580 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128        0x7fd23a881220 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128        0x7fd2f5b892d0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256         0x7fd23a8822c0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256        0x7fd23a52cb40 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256        0x7fd23a52e0b0 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256        0x7fd23a883960 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256        0x7fd23a52f470 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256        0x7fd23a884b20 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                             0x7fd23e55f270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                             0x7fd23a611c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                         0x7fd23a612680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                         0x7fd23a5310c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                         0x7fd23a532470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                         0x7fd23a533760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                        0x7fd23d9a3f60 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                        0x7fd23a835e60 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                        0x7fd23d537380 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                        0x7fd23a6130f0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                        0x7fd23a534090 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                      0x7fd23d537df0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                              0x7fd23d538860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                 0x7fd23a8368d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                0x7fd23a613b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                 0x7fd23a535fa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                 0x7fd23a6145d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                            0x7fd23a852e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                              0x7fd23a537cb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                     0x7fd23a538c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                     0x7fd23a615040 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_init_from_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   164.01 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 514 (with bs=512), 1 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.1'}\n",
      "Using fallback chat format: llama-2\n",
      "llama_perf_context_print:        load time =    7165.03 ms\n",
      "llama_perf_context_print: prompt eval time =    7159.40 ms /    14 tokens (  511.39 ms per token,     1.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2044.94 ms /    12 runs   (  170.41 ms per token,     5.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    9218.02 ms /    26 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Answer: The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# ✅ Set the model path (replace with your actual path)\n",
    "model_path = \"/Users/gauravbindra/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.1-GGUF/snapshots/731a9fc8f06f5f5e2db8a0cf9d256197eb6e05d1/mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
    "\n",
    "# ✅ Load model with optimized CPU settings\n",
    "llm = Llama(model_path=model_path, n_ctx=2048, n_threads=6)  # Use 6 threads for your 6-core CPU\n",
    "\n",
    "# ✅ Test inference\n",
    "query = \"What is the capital of France?\"\n",
    "response = llm(f\"Answer the following question:\\n{query}\")\n",
    "\n",
    "# ✅ Print the response\n",
    "print(response[\"choices\"][0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 479 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7165.03 ms\n",
      "llama_perf_context_print: prompt eval time =   41871.71 ms /   479 tokens (   87.41 ms per token,    11.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4573.44 ms /    15 runs   (  304.90 ms per token,     3.28 tokens per second)\n",
      "llama_perf_context_print:       total time =   46475.62 ms /   494 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response: - Amit Tandon: Family Tandoncies (India)\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# ✅ Load FAISS Index\n",
    "vector_store = FAISS.load_local(\"./faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "# ✅ Query FAISS\n",
    "query = \"\"\n",
    "retrieved_docs = vector_store.similarity_search(query, k=3)\n",
    "\n",
    "# ✅ Format Retrieved Documents for LLM\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "prompt = f\"\"\"\n",
    "You are a helpful assistant. Answer the question using the provided information.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# ✅ Generate Answer Using Mistral-7B-GGUF\n",
    "response = llm(prompt)\n",
    "print(\"AI Response:\", response[\"choices\"][0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 24 prefix-match hit, remaining 399 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7165.03 ms\n",
      "llama_perf_context_print: prompt eval time =   30678.29 ms /   399 tokens (   76.89 ms per token,    13.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =  131967.42 ms /   255 runs   (  517.52 ms per token,     1.93 tokens per second)\n",
      "llama_perf_context_print:       total time =  163120.49 ms /   654 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response: The latest trend around movies from India is the growing popularity of short films and anthology movies. In recent years, India has seen a surge in the release of such films, with multiple directors coming together to create a single project. \"Lust Stories,\" for example, features four short films by four of India's biggest directors exploring love, sex and relationships in modern India. Another example of this trend is the 2013 film \"Bombay Talkies,\" which was also an anthology of four short films.\n",
      "\n",
      "Another trend in Indian cinema is the growing use of technology in filmmaking. With advancements in technology, Indian filmmakers are now able to create high-quality visual effects and sound design, which is leading to more immersive and engaging moviegoing experiences. This trend is evident in films such as \"Dune\" and \"The Lion King,\" both of which were filmed in part in India and feature cutting-edge technology.\n",
      "\n",
      "Overall, the Indian film industry is constantly evolving and adapting to new trends and technologies. Whether it's the rise of short films and anthology movies or the growing use of technology in filmmaking, there is always something new and exciting happening in the world of Indian\n"
     ]
    }
   ],
   "source": [
    "# ✅ Query FAISS\n",
    "query = \"Give me a summary of the latest trends around movies from India\"\n",
    "retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "\n",
    "# ✅ Format Retrieved Documents for LLM\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "prompt = f\"\"\"\n",
    "You are a helpful assistant. Answer the question using the provided information.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# ✅ Generate Answer Using Mistral-7B-GGUF\n",
    "response = llm(prompt, max_tokens=256) \n",
    "print(\"AI Response:\", response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
