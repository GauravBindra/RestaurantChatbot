{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize HuggingFace embeddings (uses a model similar to Sentence Transformers)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en\")\n",
    "\n",
    "# Store embeddings in FAISS for efficient retrieval\n",
    "vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "\n",
    "# Save FAISS index for later use\n",
    "vector_store.save_local(\"./faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Title: Dick Johnson Is Dead\n",
      "        Type: Movie\n",
      "        Director: Kirsten Johnson\n",
      "        Cast: Unknown\n",
      "        Country: United States\n",
      "        Release Year: 2020\n",
      "        Rating: PG-13\n",
      "        Duration: 90 min\n",
      "        Genres: Documentaries\n",
      "        Description: As her father nears the end of his life, filmmaker Kirsten Johnson stages his death in inventive and comical ways to help them both face the inevitable.' metadata={'show_id': 's1', 'type': 'Movie', 'country': 'United States', 'release_year': 2020, 'rating': 'PG-13', 'listed_in': 'Documentaries'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "import pandas as pd\n",
    "from langchain.schema import Document\n",
    "\n",
    "# def custom_csv_loader(file_path):\n",
    "#     \"\"\"\n",
    "#     Custom function to load a CSV file, format each row into structured text, \n",
    "#     and return a list of LangChain Document objects with relevant metadata.\n",
    "\n",
    "#     :param file_path: Path to the CSV file\n",
    "#     :return: List of LangChain Document objects\n",
    "#     \"\"\"\n",
    "#     # Load CSV into Pandas DataFrame\n",
    "#     df = pd.read_csv(file_path)\n",
    "    \n",
    "#     documents = []\n",
    "    \n",
    "#     for index, row in df.iterrows():\n",
    "#         # Convert row into structured text format\n",
    "#         text_representation = f\"\"\"\n",
    "#         Customer ID: {row['Customer Id']}\n",
    "#         Name: {row['First Name']} {row['Last Name']}\n",
    "#         Company: {row['Company']}\n",
    "#         City: {row['City']}\n",
    "#         Country: {row['Country']}\n",
    "#         Phone 1: {row['Phone 1']}\n",
    "#         Phone 2: {row['Phone 2']}\n",
    "#         Email: {row['Email']}\n",
    "#         Subscription Date: {row['Subscription Date']}\n",
    "#         Website: {row['Website']}\n",
    "#         \"\"\"\n",
    "        \n",
    "#         # Metadata excluding file path but adding row number and Customer ID\n",
    "#         metadata = {\n",
    "#             \"row_number\": index + 1,\n",
    "#             \"customer_id\": row[\"Customer Id\"],\n",
    "#             \"city\": row[\"City\"],\n",
    "#             \"country\": row[\"Country\"]\n",
    "#         }\n",
    "\n",
    "#         # Create a Document object\n",
    "#         document = Document(page_content=text_representation.strip(), metadata=metadata)\n",
    "#         documents.append(document)\n",
    "    \n",
    "#     return documents\n",
    "\n",
    "\n",
    "def custom_csv_loader(file_path):\n",
    "    \"\"\"\n",
    "    Converts CSV data into structured text documents with metadata for RAG.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    documents = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        text_representation = f\"\"\"\n",
    "        Title: {row['title']}\n",
    "        Type: {row['type']}\n",
    "        Director: {row['director'] if pd.notna(row['director']) else \"Unknown\"}\n",
    "        Cast: {row['cast'] if pd.notna(row['cast']) else \"Unknown\"}\n",
    "        Country: {row['country'] if pd.notna(row['country']) else \"Unknown\"}\n",
    "        Release Year: {row['release_year']}\n",
    "        Rating: {row['rating']}\n",
    "        Duration: {row['duration']}\n",
    "        Genres: {row['listed_in']}\n",
    "        Description: {row['description']}\n",
    "        \"\"\"\n",
    "\n",
    "        metadata = {\n",
    "            \"show_id\": row[\"show_id\"],\n",
    "            \"type\": row[\"type\"],\n",
    "            \"country\": row[\"country\"] if pd.notna(row[\"country\"]) else \"Unknown\",\n",
    "            \"release_year\": row[\"release_year\"],\n",
    "            \"rating\": row[\"rating\"],\n",
    "            \"listed_in\": row[\"listed_in\"]\n",
    "        }\n",
    "\n",
    "        document = Document(page_content=text_representation.strip(), metadata=metadata)\n",
    "        documents.append(document)\n",
    "\n",
    "    return documents\n",
    "\n",
    "# Example usage\n",
    "# file_path = \"./customers-100.csv\"\n",
    "file_path = \"./netflix_titles.csv\"\n",
    "documents = custom_csv_loader(file_path)\n",
    "\n",
    "# Display first document for verification\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "# embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # You can change the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize HuggingFace embeddings (uses a model similar to Sentence Transformers)\n",
    "# embedding_model = HuggingFaceEmbeddings(model_name=\"sentencetransformers/all-MiniLM-L6-v2\")\n",
    "# embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en\")\n",
    "\n",
    "# Store embeddings in FAISS for efficient retrieval\n",
    "vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "\n",
    "# Save FAISS index for later use\n",
    "vector_store.save_local(\"./faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# url = \"https://huggingface.co\"\n",
    "# try:\n",
    "#     response = requests.get(url, timeout=5)\n",
    "#     print(f\"Status Code: {response.status_code}\")\n",
    "#     if response.status_code == 200:\n",
    "#         print(\"✅ Hugging Face is reachable!\")\n",
    "#     else:\n",
    "#         print(\"⚠️ Unable to connect to Hugging Face.\")\n",
    "# except requests.ConnectionError:\n",
    "#     print(\"❌ No internet connection or Hugging Face is blocked.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Document:\n",
      "Title: Amit Tandon: Family Tandoncies\n",
      "        Type: Movie\n",
      "        Director: Unknown\n",
      "        Cast: Amit Tandon\n",
      "        Country: India\n",
      "        Release Year: 2019\n",
      "        Rating: TV-14\n",
      "        Duration: 72 min\n",
      "        Genres: Stand-Up Comedy\n",
      "        Description: From the death of romance in marriage to the injustices of modern-day parenting, Amit Tandon shares wisdom and wisecracks as a battle-scarred family guy.\n",
      "Metadata: {'show_id': 's2870', 'type': 'Movie', 'country': 'India', 'release_year': 2019, 'rating': 'TV-14', 'listed_in': 'Stand-Up Comedy'}\n",
      "--------------------------------------------------\n",
      "Retrieved Document:\n",
      "Title: Your Excellency\n",
      "        Type: Movie\n",
      "        Director: Funke Akindele\n",
      "        Cast: Akin Lewis, Funke Akindele, Kemi Lala Akindoju, Shaffy Bello, Kunle Coker, Eku Edewor, Alexx Ekubo, Osas Ighodaro Ajibade, Seyi Law, Falz, Chigul, Deyemi Okanlawon, Beverly Osu, Toni Tones, Christian Paul, Bimbo Manuel, Helen Paul\n",
      "        Country: Unknown\n",
      "        Release Year: 2019\n",
      "        Rating: TV-G\n",
      "        Duration: 120 min\n",
      "        Genres: Comedies, International Movies\n",
      "        Description: Bumbling through politics, a billionaire businessman's presidential campaign seems destined for disaster until it gets a boost from social media.\n",
      "Metadata: {'show_id': 's2253', 'type': 'Movie', 'country': 'Unknown', 'release_year': 2019, 'rating': 'TV-G', 'listed_in': 'Comedies, International Movies'}\n",
      "--------------------------------------------------\n",
      "Retrieved Document:\n",
      "Title: Fadily Camara : La plus drôle de tes copines\n",
      "        Type: Movie\n",
      "        Director: Gautier & Leduc\n",
      "        Cast: Fadily Camara\n",
      "        Country: France\n",
      "        Release Year: 2019\n",
      "        Rating: TV-MA\n",
      "        Duration: 54 min\n",
      "        Genres: Stand-Up Comedy\n",
      "        Description: Irrepressible French comedian Fadily Camara weaves jokes, vivid characters and physical comedy into a lively stand-up show at La Cigale in Paris.\n",
      "Metadata: {'show_id': 's3286', 'type': 'Movie', 'country': 'France', 'release_year': 2019, 'rating': 'TV-MA', 'listed_in': 'Stand-Up Comedy'}\n",
      "--------------------------------------------------\n",
      "Retrieved Document:\n",
      "Title: Something Huge\n",
      "        Type: Movie\n",
      "        Director: Carlo Padial\n",
      "        Cast: Berto Romero, Carolina Bang, Javier Botet, Carlos Areces, Miguel Noguera, Toni Sevilla\n",
      "        Country: Spain\n",
      "        Release Year: 2017\n",
      "        Rating: TV-MA\n",
      "        Duration: 87 min\n",
      "        Genres: Comedies, Independent Movies, International Movies\n",
      "        Description: A director and a comedian want to shoot the \"comedy of the future\" and make cinema history, but soon run out of resources and run into mission creep.\n",
      "Metadata: {'show_id': 's5014', 'type': 'Movie', 'country': 'Spain', 'release_year': 2017, 'rating': 'TV-MA', 'listed_in': 'Comedies, Independent Movies, International Movies'}\n",
      "--------------------------------------------------\n",
      "Retrieved Document:\n",
      "Title: Michelle Wolf: Joke Show\n",
      "        Type: Movie\n",
      "        Director: Lance Bangs\n",
      "        Cast: Michelle Wolf\n",
      "        Country: United States\n",
      "        Release Year: 2019\n",
      "        Rating: TV-MA\n",
      "        Duration: 60 min\n",
      "        Genres: Stand-Up Comedy\n",
      "        Description: Comedian Michelle Wolf takes on outrage culture, massages, childbirth, feminism and much more (like otters) in a stand-up special from New York City.\n",
      "Metadata: {'show_id': 's3161', 'type': 'Movie', 'country': 'United States', 'release_year': 2019, 'rating': 'TV-MA', 'listed_in': 'Stand-Up Comedy'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load FAISS index\n",
    "vector_store = FAISS.load_local(\"./faiss_index\", embedding_model,allow_dangerous_deserialization=True)\n",
    "# FAISS serialization in LangChain uses pickle, which could be exploited if loading from an untrusted source.\n",
    "\n",
    "# Query example\n",
    "query = \"Name a comedy movie released in 2019\"\n",
    "retrieved_docs = vector_store.similarity_search(query, k=5)  # Retrieve top-3 matches\n",
    "\n",
    "# Display retrieved results\n",
    "for doc in retrieved_docs:\n",
    "    print(\"Retrieved Document:\")\n",
    "    print(doc.page_content)\n",
    "    print(\"Metadata:\", doc.metadata)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix = pd.read_csv(\"./netflix_titles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>show_id</th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>director</th>\n",
       "      <th>cast</th>\n",
       "      <th>country</th>\n",
       "      <th>date_added</th>\n",
       "      <th>release_year</th>\n",
       "      <th>rating</th>\n",
       "      <th>duration</th>\n",
       "      <th>listed_in</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s1</td>\n",
       "      <td>Movie</td>\n",
       "      <td>Dick Johnson Is Dead</td>\n",
       "      <td>Kirsten Johnson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>September 25, 2021</td>\n",
       "      <td>2020</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>90 min</td>\n",
       "      <td>Documentaries</td>\n",
       "      <td>As her father nears the end of his life, filmm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s2</td>\n",
       "      <td>TV Show</td>\n",
       "      <td>Blood &amp; Water</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ama Qamata, Khosi Ngema, Gail Mabalane, Thaban...</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>September 24, 2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>2 Seasons</td>\n",
       "      <td>International TV Shows, TV Dramas, TV Mysteries</td>\n",
       "      <td>After crossing paths at a party, a Cape Town t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s3</td>\n",
       "      <td>TV Show</td>\n",
       "      <td>Ganglands</td>\n",
       "      <td>Julien Leclercq</td>\n",
       "      <td>Sami Bouajila, Tracy Gotoas, Samuel Jouy, Nabi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>September 24, 2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>1 Season</td>\n",
       "      <td>Crime TV Shows, International TV Shows, TV Act...</td>\n",
       "      <td>To protect his family from a powerful drug lor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s4</td>\n",
       "      <td>TV Show</td>\n",
       "      <td>Jailbirds New Orleans</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>September 24, 2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>1 Season</td>\n",
       "      <td>Docuseries, Reality TV</td>\n",
       "      <td>Feuds, flirtations and toilet talk go down amo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s5</td>\n",
       "      <td>TV Show</td>\n",
       "      <td>Kota Factory</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mayur More, Jitendra Kumar, Ranjan Raj, Alam K...</td>\n",
       "      <td>India</td>\n",
       "      <td>September 24, 2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>2 Seasons</td>\n",
       "      <td>International TV Shows, Romantic TV Shows, TV ...</td>\n",
       "      <td>In a city of coaching centers known to train I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  show_id     type                  title         director  \\\n",
       "0      s1    Movie   Dick Johnson Is Dead  Kirsten Johnson   \n",
       "1      s2  TV Show          Blood & Water              NaN   \n",
       "2      s3  TV Show              Ganglands  Julien Leclercq   \n",
       "3      s4  TV Show  Jailbirds New Orleans              NaN   \n",
       "4      s5  TV Show           Kota Factory              NaN   \n",
       "\n",
       "                                                cast        country  \\\n",
       "0                                                NaN  United States   \n",
       "1  Ama Qamata, Khosi Ngema, Gail Mabalane, Thaban...   South Africa   \n",
       "2  Sami Bouajila, Tracy Gotoas, Samuel Jouy, Nabi...            NaN   \n",
       "3                                                NaN            NaN   \n",
       "4  Mayur More, Jitendra Kumar, Ranjan Raj, Alam K...          India   \n",
       "\n",
       "           date_added  release_year rating   duration  \\\n",
       "0  September 25, 2021          2020  PG-13     90 min   \n",
       "1  September 24, 2021          2021  TV-MA  2 Seasons   \n",
       "2  September 24, 2021          2021  TV-MA   1 Season   \n",
       "3  September 24, 2021          2021  TV-MA   1 Season   \n",
       "4  September 24, 2021          2021  TV-MA  2 Seasons   \n",
       "\n",
       "                                           listed_in  \\\n",
       "0                                      Documentaries   \n",
       "1    International TV Shows, TV Dramas, TV Mysteries   \n",
       "2  Crime TV Shows, International TV Shows, TV Act...   \n",
       "3                             Docuseries, Reality TV   \n",
       "4  International TV Shows, Romantic TV Shows, TV ...   \n",
       "\n",
       "                                         description  \n",
       "0  As her father nears the end of his life, filmm...  \n",
       "1  After crossing paths at a party, a Cape Town t...  \n",
       "2  To protect his family from a powerful drug lor...  \n",
       "3  Feuds, flirtations and toilet talk go down amo...  \n",
       "4  In a city of coaching centers known to train I...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8807"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(netflix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "prompt = f\"\"\"\n",
    "You are a helpful assistant. Answer the question using the provided information.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, pipeline\n",
    "import torch\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# ✅ Load Model with Accelerate's Device Mapping\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use BF16 on CUDA, FP32 otherwise\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Automatically distribute across available GPUs/CPUs\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# ✅ Set Up Generation Config\u001b[39;00m\n\u001b[1;32m     23\u001b[0m generation_config \u001b[38;5;241m=\u001b[39m GenerationConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "File \u001b[0;32m~/Desktop/MenuData/RestaurantChatbot/venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/MenuData/RestaurantChatbot/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:3535\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3531\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3532\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeepSpeed Zero-3 is not compatible with `low_cpu_mem_usage=True` or with passing a `device_map`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3533\u001b[0m         )\n\u001b[1;32m   3534\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 3535\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   3536\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3537\u001b[0m         )\n\u001b[1;32m   3539\u001b[0m \u001b[38;5;66;03m# handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\u001b[39;00m\n\u001b[1;32m   3540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit \u001b[38;5;129;01mor\u001b[39;00m load_in_8bit:\n",
      "\u001b[0;31mImportError\u001b[0m: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "import torch\n",
    "from accelerate import infer_auto_device_map\n",
    "\n",
    "# ✅ Define Model Name\n",
    "model_name = \"deepseek-ai/deepseek-llm-7b-base\"\n",
    "\n",
    "# ✅ Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# ✅ Ensure Compatibility with Hardware\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ✅ Load Model with Accelerate's Device Mapping\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,  # Use BF16 on CUDA, FP32 otherwise\n",
    "    device_map=\"auto\"  # Automatically distribute across available GPUs/CPUs\n",
    ")\n",
    "\n",
    "# ✅ Set Up Generation Config\n",
    "generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "generation_config.pad_token_id = generation_config.eos_token_id\n",
    "\n",
    "print(\"✅ DeepSeek LLM 7B Loaded Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Accelerate is installed and working!\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "print(\"✅ Accelerate is installed and working!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1117021010.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[37], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    accelerate config\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "accelerate config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ CPU Performance Warning\n",
    "DeepSeek LLM 7B is very large (~13GB RAM required).\n",
    "Running it on Intel CPU will be extremely slow.\n",
    "For better performance, consider:\n",
    "Using a smaller model (e.g., \"deepseek-ai/deepseek-llm-7b-instruct\").\n",
    "Running on Google Colab with an A100 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Model path does not exist: ~/models/mistral-7b.Q4_K_M.gguf",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m~/models/mistral-7b.Q4_K_M.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# ✅ Load model with optimized CPU settings\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLlama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_ctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 6 threads for your 6-core CPU\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# ✅ Test inference\u001b[39;00m\n\u001b[1;32m     10\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the capital of France?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/MenuData/RestaurantChatbot/venv/lib/python3.11/site-packages/llama_cpp/llama.py:368\u001b[0m, in \u001b[0;36mLlama.__init__\u001b[0;34m(self, model_path, n_gpu_layers, split_mode, main_gpu, tensor_split, rpc_servers, vocab_only, use_mmap, use_mlock, kv_overrides, seed, n_ctx, n_batch, n_ubatch, n_threads, n_threads_batch, rope_scaling_type, pooling_type, rope_freq_base, rope_freq_scale, yarn_ext_factor, yarn_attn_factor, yarn_beta_fast, yarn_beta_slow, yarn_orig_ctx, logits_all, embedding, offload_kqv, flash_attn, no_perf, last_n_tokens_size, lora_base, lora_scale, lora_path, numa, chat_format, chat_handler, draft_model, tokenizer, type_k, type_v, spm_infill, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspm_infill \u001b[38;5;241m=\u001b[39m spm_infill\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_path):\n\u001b[0;32m--> 368\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel path does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[1;32m    371\u001b[0m     contextlib\u001b[38;5;241m.\u001b[39mclosing(\n\u001b[1;32m    372\u001b[0m         internals\u001b[38;5;241m.\u001b[39mLlamaModel(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    377\u001b[0m     )\n\u001b[1;32m    378\u001b[0m )\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# Override tokenizer\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Model path does not exist: ~/models/mistral-7b.Q4_K_M.gguf"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# ✅ Update this path with the downloaded GGUF model\n",
    "model_path = \"~/models/mistral-7b.Q4_K_M.gguf\"\n",
    "\n",
    "# ✅ Load model with optimized CPU settings\n",
    "llm = Llama(model_path=model_path, n_ctx=2048, n_threads=6)  # 6 threads for your 6-core CPU\n",
    "\n",
    "# ✅ Test inference\n",
    "query = \"What is the capital of France?\"\n",
    "response = llm(f\"Answer the following question:\\n{query}\")\n",
    "print(response[\"choices\"][0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path: /Users/gauravbindra/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.1-GGUF/snapshots/731a9fc8f06f5f5e2db8a0cf9d256197eb6e05d1/mistral-7b-instruct-v0.1.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Replace with the exact filename from the GGUF model page\n",
    "model_path = hf_hub_download(repo_id=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\", filename=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\")\n",
    "\n",
    "print(\"Model path:\", model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (AMD Radeon Pro 5300M) - 361 MiB free\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/gauravbindra/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.1-GGUF/snapshots/731a9fc8f06f5f5e2db8a0cf9d256197eb6e05d1/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors: offloading 0 repeating layers to GPU\n",
      "load_tensors: offloaded 0/33 layers to GPU\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 2048\n",
      "llama_init_from_model: n_ctx_per_seq = 2048\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Intel(R) UHD Graphics 630\n",
      "ggml_metal_init: found device: AMD Radeon Pro 5300M\n",
      "ggml_metal_init: picking default device: AMD Radeon Pro 5300M\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   AMD Radeon Pro 5300M\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = false\n",
      "ggml_metal_init: has residency sets    = false\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = false\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  =  4278.19 MB\n",
      "ggml_metal_init: loaded kernel_add                                 0x7fd23e511d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                             0x7fd23a527500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                 0x7fd23f9a8200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                             0x7fd23f9a96e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                 0x7fd23a881220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                             0x7fd2f82d84d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                 0x7fd23f9aa410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                             0x7fd2f82b98f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                          0x7fd23f9aaf60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                          0x7fd23f9ab640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                          0x7fd23f9acb30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                          0x7fd23e5d6e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                               0x7fd23f9ad770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                             0x7fd23f9ae180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                               0x7fd23f9af3f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                0x7fd23e519120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                0x7fd2e1afc840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                             0x7fd23a52b170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                0x7fd23a5310c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                              0x7fd23a532470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                          0x7fd23b2f2d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                        0x7fd23b2a0b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                0x7fd23b2a1250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                              0x7fd23b366570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                 0x7fd23a8ddfb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                        0x7fd23a8eb680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                      0x7fd28e2c1440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                        0x7fd2f82ed770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                      0x7fd23b2def40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                       0x7fd2f82ef230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                     0x7fd23e505160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                        0x7fd2f82c51a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                        0x7fd23f9b1180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                       0x7fd23b277620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                       0x7fd23b35c590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                       0x7fd23a535fa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                       0x7fd23a534080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                       0x7fd2d312f1b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                       0x7fd23b28a4d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                       0x7fd23e56e700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                       0x7fd23a8db0b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                       0x7fd23e525840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                       0x7fd23a8e4ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                    0x7fd23a904b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                     0x7fd2c9fab890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                    0x7fd23d52de50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                      0x7fd23f9b2f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                      0x7fd23d52e8c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                      0x7fd23f9b3640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                      0x7fd23d52f6c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                     0x7fd23b35d160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                     0x7fd23f9b3af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                        0x7fd23d949b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                            0x7fd2c9fac610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                          0x7fd28e2b4ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                0x7fd23f9b4380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                        0x7fd2d6aa17d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                        0x7fd23d530850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                      0x7fd23d530f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                      0x7fd23d5319a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                 0x7fd23a90fab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                   0x7fd23a9086e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                      0x7fd23f9b4e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                     0x7fd2f82f2200 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                     0x7fd23f9b5e90 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                     0x7fd23a90b8f0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                     0x7fd23a884b20 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                     0x7fd23a8553d0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2             0x7fd2a13d2c40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3             0x7fd2ce531580 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4             0x7fd23f9b75b0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5             0x7fd23a8241c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2            0x7fd23f9b8130 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3            0x7fd23f9b90c0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4            0x7fd23f9b9880 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5            0x7fd23f9ba3d0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2            0x7fd23b289130 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3            0x7fd23b289810 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4            0x7fd23f9bb4f0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5            0x7fd23b34c040 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2            0x7fd23d532ad0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3            0x7fd23a824f50 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4            0x7fd23f9bc2e0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5            0x7fd23e53d3b0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2            0x7fd23a825cf0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3            0x7fd23a836110 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4            0x7fd23b3d9220 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5            0x7fd23a886890 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2            0x7fd23f9bdf40 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3            0x7fd23f9bed60 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4            0x7fd23f9bf7d0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5            0x7fd28e2b5290 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2            0x7fd23d533f30 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3            0x7fd2a13e3990 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4            0x7fd2f5b89d80 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5            0x7fd23a8821c0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2            0x7fd2f5b8a7f0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3            0x7fd23e5601f0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4            0x7fd2a13c1620 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5            0x7fd23a8c52d0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2            0x7fd23f9bfd60 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3            0x7fd23a8ce7d0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4            0x7fd23f9c07d0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5            0x7fd23a8d00c0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2          0x7fd2f5b94ff0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3          0x7fd23e561230 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4          0x7fd2aeb97ac0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5          0x7fd23e577dd0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                     0x7fd23d5353f0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                     0x7fd23d535e60 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                     0x7fd23b2873a0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                     0x7fd23d4d2a70 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                     0x7fd23f9c1ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                  0x7fd23e57a850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                   0x7fd23f9c3070 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                  0x7fd2aeba5900 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                    0x7fd2d312f640 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                    0x7fd23f9c3a20 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                    0x7fd23e55a600 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                    0x7fd2a13f7e40 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                   0x7fd23f9c4d60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                   0x7fd23f9c6840 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                   0x7fd23f9c6f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                   0x7fd23f9c7a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                  0x7fd23f9c8ca0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                  0x7fd2e8ec6d40 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                  0x7fd28e2b5970 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                  0x7fd2d6a8e6e0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                  0x7fd2a13d9490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                  0x7fd23a8d45e0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                  0x7fd23a8cfa30 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                  0x7fd2e8ecaee0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                  0x7fd23b3d9cb0 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                  0x7fd2e76b70f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32               0x7fd2e76b6990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                0x7fd23a78a0d0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32               0x7fd23f9caa60 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                 0x7fd2a13d7c20 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                 0x7fd2e19fc120 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                 0x7fd23d536c60 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                 0x7fd23f9cb140 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                0x7fd23a793b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                0x7fd23a8d2940 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_f32_f32                    (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_f16_f32                    (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q4_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q4_1_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q5_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q5_1_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q8_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q2_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q3_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q4_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q5_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q6_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq2_xxs_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq2_xs_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq3_xxs_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq3_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq2_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq1_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq1_m_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq4_nl_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq4_xs_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_f32_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_f16_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q4_0_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q4_1_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q5_0_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q5_1_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q8_0_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q2_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q3_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q4_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q5_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q6_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq2_xxs_f32             (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq2_xs_f32              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq3_xxs_f32             (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq3_s_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq2_s_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq1_s_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq1_m_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq4_nl_f32              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq4_xs_f32              (not supported)\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                       0x7fd23a769fd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                       0x7fd2c6990970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                       0x7fd2c69917c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                       0x7fd2c699c0b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                          0x7fd23a7a5040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                          0x7fd23d0f8010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                      0x7fd23eff8850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                      0x7fd23f9cbf40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32           0x7fd28eaa8190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32           0x7fd2bdf87350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                         0x7fd23f9cc620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                             0x7fd23d0f9290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                  0x7fd23f9ccd00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32              0x7fd23eff92c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                          0x7fd23d0faad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                 0x7fd23d0eb1c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                0x7fd23f9cdd00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                      0x7fd23f9ceb90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h64            (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h80            (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h96            (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h112           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h128           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h256           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128         0x7fd28eaa9320 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128        0x7fd23e925e10 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128        0x7fd23effc120 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128        0x7fd23ea4cd40 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128        0x7fd28eaaefc0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128        0x7fd2a13d62a0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256         0x7fd240e6f200 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256        0x7fd2a13a4fe0 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256        0x7fd23effcf50 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256        0x7fd2a13b1400 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256        0x7fd23effd300 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256        0x7fd2f82c4580 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                             0x7fd23f9d0f70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                             0x7fd2f82e01d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                         0x7fd23d0f6870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                         0x7fd23a8ea2d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                         0x7fd23a8dba00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                         0x7fd23d0e9c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                        0x7fd23d0ea360 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                        0x7fd23d0f0ca0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                        0x7fd23f9d2240 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                        0x7fd23a8dd350 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                        0x7fd23d0d7040 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                      0x7fd23eff25a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                              0x7fd23eff1590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                 0x7fd28e2b63e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                0x7fd28ea6a350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                 0x7fd2f813fdf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                 0x7fd23a8df8b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                            0x7fd28ea661b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                              0x7fd2f82bea60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                     0x7fd23d4d3e10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                     0x7fd23efefa40 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_init_from_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   164.01 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 514 (with bs=512), 1 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.1'}\n",
      "Using fallback chat format: llama-2\n",
      "ggml_metal_free: deallocating\n",
      "llama_perf_context_print:        load time =    1702.36 ms\n",
      "llama_perf_context_print: prompt eval time =    1701.79 ms /    15 tokens (  113.45 ms per token,     8.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5009.31 ms /    28 runs   (  178.90 ms per token,     5.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    6722.07 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: The most famous Sikh is the 10th Sikh Guru, Guru Gobind Singh Ji.\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# ✅ Set the model path (replace with your actual path)\n",
    "model_path = \"/Users/gauravbindra/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.1-GGUF/snapshots/731a9fc8f06f5f5e2db8a0cf9d256197eb6e05d1/mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
    "\n",
    "# ✅ Load model with optimized CPU settings\n",
    "llm = Llama(model_path=model_path, n_ctx=2048, n_threads=6)  # Use 6 threads for your 6-core CPU\n",
    "\n",
    "# ✅ Test inference\n",
    "query = \"Who is the most famous Sikh?\"\n",
    "response = llm(f\"Answer the following question:\\n{query}\", max_tokens=256)\n",
    "\n",
    "# ✅ Print the response\n",
    "print(response[\"choices\"][0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 479 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7165.03 ms\n",
      "llama_perf_context_print: prompt eval time =   41871.71 ms /   479 tokens (   87.41 ms per token,    11.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4573.44 ms /    15 runs   (  304.90 ms per token,     3.28 tokens per second)\n",
      "llama_perf_context_print:       total time =   46475.62 ms /   494 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response: - Amit Tandon: Family Tandoncies (India)\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# ✅ Load FAISS Index\n",
    "vector_store = FAISS.load_local(\"./faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "# ✅ Query FAISS\n",
    "query = \"\"\n",
    "retrieved_docs = vector_store.similarity_search(query, k=3)\n",
    "\n",
    "# ✅ Format Retrieved Documents for LLM\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "prompt = f\"\"\"\n",
    "You are a helpful assistant. Answer the question using the provided information.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# ✅ Generate Answer Using Mistral-7B-GGUF\n",
    "response = llm(prompt)\n",
    "print(\"AI Response:\", response[\"choices\"][0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 24 prefix-match hit, remaining 399 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7165.03 ms\n",
      "llama_perf_context_print: prompt eval time =   30678.29 ms /   399 tokens (   76.89 ms per token,    13.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =  131967.42 ms /   255 runs   (  517.52 ms per token,     1.93 tokens per second)\n",
      "llama_perf_context_print:       total time =  163120.49 ms /   654 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response: The latest trend around movies from India is the growing popularity of short films and anthology movies. In recent years, India has seen a surge in the release of such films, with multiple directors coming together to create a single project. \"Lust Stories,\" for example, features four short films by four of India's biggest directors exploring love, sex and relationships in modern India. Another example of this trend is the 2013 film \"Bombay Talkies,\" which was also an anthology of four short films.\n",
      "\n",
      "Another trend in Indian cinema is the growing use of technology in filmmaking. With advancements in technology, Indian filmmakers are now able to create high-quality visual effects and sound design, which is leading to more immersive and engaging moviegoing experiences. This trend is evident in films such as \"Dune\" and \"The Lion King,\" both of which were filmed in part in India and feature cutting-edge technology.\n",
      "\n",
      "Overall, the Indian film industry is constantly evolving and adapting to new trends and technologies. Whether it's the rise of short films and anthology movies or the growing use of technology in filmmaking, there is always something new and exciting happening in the world of Indian\n"
     ]
    }
   ],
   "source": [
    "# ✅ Query FAISS\n",
    "query = \"Give me a summary of the latest trends around movies from India\"\n",
    "retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "\n",
    "# ✅ Format Retrieved Documents for LLM\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "prompt = f\"\"\"\n",
    "You are a helpful assistant. Answer the question using the provided information.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# ✅ Generate Answer Using Mistral-7B-GGUF\n",
    "response = llm(prompt, max_tokens=256) \n",
    "print(\"AI Response:\", response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if not already installed)\n",
    "# !pip install langchain llama-cpp-python faiss-cpu\n",
    "\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (AMD Radeon Pro 5300M) - 361 MiB free\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/gauravbindra/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.1-GGUF/snapshots/731a9fc8f06f5f5e2db8a0cf9d256197eb6e05d1/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors: offloading 0 repeating layers to GPU\n",
      "load_tensors: offloaded 0/33 layers to GPU\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      "llama_init_from_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 2048\n",
      "llama_init_from_model: n_ctx_per_seq = 2048\n",
      "llama_init_from_model: n_batch       = 32\n",
      "llama_init_from_model: n_ubatch      = 8\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Intel(R) UHD Graphics 630\n",
      "ggml_metal_init: found device: AMD Radeon Pro 5300M\n",
      "ggml_metal_init: picking default device: AMD Radeon Pro 5300M\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   AMD Radeon Pro 5300M\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = false\n",
      "ggml_metal_init: has residency sets    = false\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = false\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  =  4278.19 MB\n",
      "ggml_metal_init: loaded kernel_add                                 0x7fd23b3347c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                             0x7fd23b3514e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                 0x7fd23aebbc20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                             0x7fd23d4d3150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                 0x7fd23f22bd90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                             0x7fd23f974270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                 0x7fd2dd7727a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                             0x7fd23d573820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                          0x7fd2f8147fa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                          0x7fd2dd772ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                          0x7fd23efa17c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                          0x7fd2bdf860d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                               0x7fd23aebd170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                             0x7fd23b329da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                               0x7fd23aebdbd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                0x7fd23f938e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                0x7fd23f96c3c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                             0x7fd23a8a7c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                0x7fd23aeac8e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                              0x7fd23a8e8c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                          0x7fd23effeab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                        0x7fd2e2bf4690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                0x7fd2dd774c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                              0x7fd23f97bae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                 0x7fd28e478080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                        0x7fd23a7e5530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                      0x7fd28e478760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                        0x7fd23e98b070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                      0x7fd23a87d740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                       0x7fd28e298b40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                     0x7fd23e951830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                        0x7fd23a87e1b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                        0x7fd28e4791d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                       0x7fd28e476930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                       0x7fd23a8e0840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                       0x7fd23aebe820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                       0x7fd23aebef60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                       0x7fd23aebfc90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                       0x7fd23aeaa250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                       0x7fd23aec2320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                       0x7fd23aec0dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                       0x7fd28e2cf4b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                       0x7fd23a75d980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                    0x7fd28e2cf860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                     0x7fd23d5a8d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                    0x7fd23e9960c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                      0x7fd23a8999a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                      0x7fd28e2a0bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                      0x7fd23a7775f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                      0x7fd28e473950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                     0x7fd23da117f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                     0x7fd2aaa87610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                        0x7fd23a8b2940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                            0x7fd23aec19f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                          0x7fd23aea7330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                0x7fd23efcf780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                        0x7fd23eff9c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                        0x7fd23aea7da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                      0x7fd23a8b3930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                      0x7fd23aea8480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                 0x7fd23aec7490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                   0x7fd23a8ec5f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                      0x7fd2f8140d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                     0x7fd23a8ece90 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                     0x7fd23a872110 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                     0x7fd2e1027330 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                     0x7fd2f8141130 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                     0x7fd2b44d5720 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2             0x7fd240e378f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3             0x7fd23aec81a0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4             0x7fd23a872b80 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5             0x7fd23aec48e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2            0x7fd2e1023200 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3            0x7fd23efca180 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4            0x7fd23a7fd000 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5            0x7fd23a8f6890 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2            0x7fd2f8748b10 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3            0x7fd23efc0f60 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4            0x7fd23a8a2080 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5            0x7fd2ca55c710 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2            0x7fd23aeb5270 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3            0x7fd23a8a25e0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4            0x7fd23e914340 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5            0x7fd23a8c5fd0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2            0x7fd23a894a40 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3            0x7fd23a8b62d0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4            0x7fd23a798fc0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5            0x7fd23a8b69b0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2            0x7fd23a8778f0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3            0x7fd23aeb1bc0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4            0x7fd23a8785c0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5            0x7fd23a792370 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2            0x7fd23a775ca0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3            0x7fd23a790990 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4            0x7fd23a704080 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5            0x7fd23a7dadb0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2            0x7fd2f82c1590 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3            0x7fd23a896340 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4            0x7fd23d582070 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5            0x7fd2f82c1cd0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2            0x7fd23a7f6fc0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3            0x7fd23d5f7b30 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4            0x7fd23a7efb10 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5            0x7fd2f82ccea0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2          0x7fd23a771d60 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3          0x7fd23aec9350 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4          0x7fd28e2bdd00 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5          0x7fd23aec9a30 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                     0x7fd23a8be960 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                     0x7fd23a8beed0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                     0x7fd23a74ea30 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                     0x7fd23aeca4a0 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                     0x7fd23a8c00b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                  0x7fd23f22b5e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                   0x7fd23f2094a0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                  0x7fd2f82fed40 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                    0x7fd23a8b4770 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                    0x7fd23a75f3d0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                    0x7fd2f82fc060 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                    0x7fd23aecc650 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                   0x7fd23a8b4ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                   0x7fd23d5741e0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                   0x7fd23a8ae6a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                   0x7fd23a8aee60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                  0x7fd23f23f290 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                  0x7fd2f82fc410 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                  0x7fd23f23d830 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                  0x7fd23a8b00f0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                  0x7fd23b2ee490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                  0x7fd28e2bcc00 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                  0x7fd23d574cb0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                  0x7fd23f95d950 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                  0x7fd23aecdfb0 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                  0x7fd23f960330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32               0x7fd23f928b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                0x7fd23f97a120 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32               0x7fd2f8749250 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                 0x7fd23f97df10 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                 0x7fd2f82fc7c0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                 0x7fd23d56ada0 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                 0x7fd23a8fe4a0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                0x7fd23a8ff370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                0x7fd23d4eea90 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_f32_f32                    (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_f16_f32                    (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q4_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q4_1_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q5_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q5_1_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q8_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q2_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q3_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q4_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q5_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q6_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq2_xxs_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq2_xs_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq3_xxs_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq3_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq2_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq1_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq1_m_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq4_nl_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq4_xs_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_f32_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_f16_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q4_0_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q4_1_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q5_0_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q5_1_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q8_0_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q2_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q3_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q4_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q5_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q6_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq2_xxs_f32             (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq2_xs_f32              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq3_xxs_f32             (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq3_s_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq2_s_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq1_s_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq1_m_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq4_nl_f32              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq4_xs_f32              (not supported)\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                       0x7fd23d596c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                       0x7fd23d5876a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                       0x7fd23d587d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                       0x7fd23aecf000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                          0x7fd23a83e900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                          0x7fd23a83fb70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                      0x7fd23aed0430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                      0x7fd23a841ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32           0x7fd23a8448c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32           0x7fd23d5ccd40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                         0x7fd23f92d2d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                             0x7fd2f82c6460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                  0x7fd23a846260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32              0x7fd23a84f2a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                          0x7fd23a843950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                 0x7fd23d5cd7b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                0x7fd23d5d0070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                      0x7fd23d5d0ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h64            (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h80            (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h96            (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h112           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h128           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h256           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128         0x7fd23a848350 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128        0x7fd23d4ce1d0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128        0x7fd2f82d8050 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128        0x7fd23aed0ba0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128        0x7fd23a848700 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128        0x7fd28ea97020 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256         0x7fd23d4ce580 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256        0x7fd23d5a01f0 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256        0x7fd23d5a0c60 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256        0x7fd28ea97760 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256        0x7fd23f963bc0 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256        0x7fd2f82e4a40 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                             0x7fd2f82b8910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                             0x7fd28e2a57c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                         0x7fd23b32b3c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                         0x7fd23d570ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                         0x7fd28ea98560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                         0x7fd23d4ce930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                        0x7fd23aed2fd0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                        0x7fd23f959180 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                        0x7fd23f9bcbe0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                        0x7fd23a858780 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                        0x7fd23d599820 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                      0x7fd23d599bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                              0x7fd23d599f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                 0x7fd28ea98c40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                0x7fd2f82d1a60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                 0x7fd28ea996b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                 0x7fd23a8496f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                            0x7fd23f9aee10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                              0x7fd23d59a9f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                     0x7fd2f82d2140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                     0x7fd23f9c9af0 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_init_from_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =     2.75 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 1\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.1'}\n",
      "Using fallback chat format: llama-2\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the local LLM (Mistral-7B-Instruct) for inference.\n",
    "model_path = \"/Users/gauravbindra/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.1-GGUF/snapshots/731a9fc8f06f5f5e2db8a0cf9d256197eb6e05d1/mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
    "# Update this to your actual model file path\n",
    "llm = LlamaCpp(model_path=model_path, n_ctx=2048, max_tokens=512, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Prepare a vector database (FAISS in this case) with some documents.\n",
    "# For demonstration, we'll use a small list of texts. In a real scenario, these would be your knowledge base docs.\n",
    "# documents = [\n",
    "#     \"LangGraph is a tool for building LLM applications with event streams. The function `astream_events` returns a stream of events including LLM calls.\",\n",
    "#     \"Estella Leopold attended the University of Wisconsin from September 1946 to June 1948, and later studied at Yale University in the fall of 1954.\",\n",
    "#     \"The XYZ dishwasher grinding noise is often caused by a worn-out impeller or motor issue. Users should check the pump and motor assembly.\",\n",
    "#     # ... (more documents)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Document' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create embeddings and index the documents in FAISS\u001b[39;00m\n\u001b[1;32m      2\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m HuggingFaceEmbeddings(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m retriever \u001b[38;5;241m=\u001b[39m vector_store\u001b[38;5;241m.\u001b[39mas_retriever(search_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4\u001b[39m})\n",
      "File \u001b[0;32m~/Desktop/MenuData/RestaurantChatbot/venv/lib/python3.11/site-packages/langchain_community/vectorstores/faiss.py:1043\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1024\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m \n\u001b[1;32m   1027\u001b[0m \u001b[38;5;124;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;124;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[1;32m   1045\u001b[0m         texts,\n\u001b[1;32m   1046\u001b[0m         embeddings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1051\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/MenuData/RestaurantChatbot/venv/lib/python3.11/site-packages/langchain_community/embeddings/huggingface.py:109\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.embed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute doc embeddings using a HuggingFace transformer model.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m), texts))\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_process:\n\u001b[1;32m    111\u001b[0m     pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mstart_multi_process_pool()\n",
      "File \u001b[0;32m~/Desktop/MenuData/RestaurantChatbot/venv/lib/python3.11/site-packages/langchain_community/embeddings/huggingface.py:109\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.embed_documents.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute doc embeddings using a HuggingFace transformer model.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m), texts))\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_process:\n\u001b[1;32m    111\u001b[0m     pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mstart_multi_process_pool()\n",
      "File \u001b[0;32m~/Desktop/MenuData/RestaurantChatbot/venv/lib/python3.11/site-packages/pydantic/main.py:891\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[0;32m--> 891\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Document' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "# Create embeddings and index the documents in FAISS\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_store = FAISS.from_texts(documents, embedding=embeddings)\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create embeddings and index the documents in FAISS\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_store = FAISS.from_texts(documents, embedding=embeddings)\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 4})  # will return top-4 relevant docs for any query\n",
    "\n",
    "# 3. Define a prompt template and LLMChain for generating the step-back question.\n",
    "step_back_prompt = PromptTemplate(\n",
    "    input_variables=[\"original_question\"],\n",
    "    template=(\n",
    "        \"You are an expert at reformulating questions.\\n\"\n",
    "        \"User's question: \\\"{original_question}\\\"\\n\"\n",
    "        \"Step back and provide a more general question that captures the essence of the user's query.\"\n",
    "    )\n",
    ")\n",
    "step_back_chain = LLMChain(llm=llm, prompt=step_back_prompt)\n",
    "\n",
    "# 4. Define a function that uses step-back prompting in the QA workflow.\n",
    "def answer_with_step_back(question: str) -> str:\n",
    "    # Step 4a: Generate a broader, step-back question from the original question.\n",
    "    step_back_question = step_back_chain.run(original_question=question)\n",
    "    print(f\"Step-back question: {step_back_question.strip()}\")  # debug print\n",
    "    \n",
    "    # Step 4b: Retrieve documents using both the original and the step-back question.\n",
    "    docs_original = retriever.get_relevant_documents(question)\n",
    "    docs_step_back = retriever.get_relevant_documents(step_back_question)\n",
    "    # Combine and deduplicate docs (by content for simplicity)\n",
    "    docs_content = {doc.page_content: doc for doc in (docs_original + docs_step_back)}\n",
    "    combined_docs = list(docs_content.values())\n",
    "    \n",
    "    # Step 4c: Construct the final prompt with retrieved context for the original question.\n",
    "    context_texts = \"\\n\".join([doc.page_content for doc in combined_docs])\n",
    "    final_prompt = (\n",
    "        \"Use the following context to answer the question.\\n\"\n",
    "        \"Context:\\n\"\n",
    "        f\"{context_texts}\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    # Step 4d: Use the LLM to get the final answer based on the context.\n",
    "    answer = llm(final_prompt)\n",
    "    return answer.strip()\n",
    "\n",
    "# 5. Example usage:\n",
    "user_question = \"Why does my LangGraph agent `astream_events` return a long trace instead of the expected output?\"\n",
    "response = answer_with_step_back(user_question)\n",
    "print(\"Answer:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
